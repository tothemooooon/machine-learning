{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据预处理 - MNIST数字识别\n",
    "\n",
    "## 概述\n",
    "本notebook专门用于MNIST数据集的加载、预处理和探索性数据分析。为神经网络训练准备高质量的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"库导入完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据加载和基础信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_data():\n",
    "    \"\"\"加载MNIST数据集\"\"\"\n",
    "    print(\"正在加载MNIST数据集...\")\n",
    "    \n",
    "    # 从OpenML加载MNIST数据\n",
    "    mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "    X, y = mnist.data, mnist.target.astype(int)\n",
    "    \n",
    "    print(f\"数据集基本信息:\")\n",
    "    print(f\"  - 特征数量: {X.shape[1]}\")\n",
    "    print(f\"  - 样本数量: {X.shape[0]}\")\n",
    "    print(f\"  - 数据类型: {X.dtype}\")\n",
    "    print(f\"  - 标签类型: {type(y[0])}\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# 加载数据\n",
    "X_raw, y_raw = load_mnist_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据质量检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_quality(X, y):\n",
    "    \"\"\"检查数据质量\"\"\"\n",
    "    print(\"\\n=== 数据质量检查 ===\")\n",
    "    \n",
    "    # 检查缺失值\n",
    "    missing_values = np.isnan(X).sum()\n",
    "    print(f\"缺失值数量: {missing_values}\")\n",
    "    \n",
    "    # 检查数据范围\n",
    "    print(f\"\\n像素值统计:\")\n",
    "    print(f\"  - 最小值: {X.min()}\")\n",
    "    print(f\"  - 最大值: {X.max()}\")\n",
    "    print(f\"  - 平均值: {X.mean():.2f}\")\n",
    "    print(f\"  - 标准差: {X.std():.2f}\")\n",
    "    \n",
    "    # 检查标签分布\n",
    "    print(f\"\\n标签分布:\")\n",
    "    unique_labels, counts = np.unique(y, return_counts=True)\n",
    "    for label, count in zip(unique_labels, counts):\n",
    "        print(f\"  - 数字 {label}: {count} 个样本 ({count/len(y)*100:.1f}%)\")\n",
    "    \n",
    "    # 检查数据一致性\n",
    "    print(f\"\\n数据一致性检查:\")\n",
    "    print(f\"  - 特征数和样本数匹配: {X.shape[0] == len(y)}\")\n",
    "    print(f\"  - 所有标签都在0-9范围内: {np.all((y >= 0) & (y <= 9))}\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# 检查数据质量\n",
    "X_raw, y_raw = check_data_quality(X_raw, y_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 数据可视化分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data_distribution(X, y):\n",
    "    \"\"\"可视化数据分布\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. 标签分布柱状图\n",
    "    ax1 = axes[0, 0]\n",
    "    unique_labels, counts = np.unique(y, return_counts=True)\n",
    "    ax1.bar(unique_labels, counts, color='skyblue', alpha=0.7)\n",
    "    ax1.set_title('标签分布')\n",
    "    ax1.set_xlabel('数字')\n",
    "    ax1.set_ylabel('样本数量')\n",
    "    ax1.set_xticks(range(10))\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. 像素值分布\n",
    "    ax2 = axes[0, 1]\n",
    "    sample_pixels = X.flatten()\n",
    "    ax2.hist(sample_pixels, bins=50, color='lightgreen', alpha=0.7, density=True)\n",
    "    ax2.set_title('像素值分布')\n",
    "    ax2.set_xlabel('像素值')\n",
    "    ax2.set_ylabel('密度')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. 每个数字的平均图像\n",
    "    ax3 = axes[0, 2]\n",
    "    avg_images = []\n",
    "    for digit in range(10):\n",
    "        mask = y == digit\n",
    "        avg_image = X[mask].mean(axis=0).reshape(28, 28)\n",
    "        avg_images.append(avg_image)\n",
    "    \n",
    "    # 显示所有数字的平均图像\n",
    "    for i, avg_img in enumerate(avg_images):\n",
    "        ax3 = axes[1, i] if i < 3 else (axes[0, 2] if i == 3 else None)\n",
    "        if ax3 is not None and i < 3:\n",
    "            ax3.imshow(avg_img, cmap='gray')\n",
    "            ax3.set_title(f'数字 {i} 的平均图像')\n",
    "            ax3.axis('off')\n",
    "    \n",
    "    # 4. 像素值热力图 (第一个样本)\n",
    "    ax4 = axes[1, 0]\n",
    "    first_image = X[0].reshape(28, 28)\n",
    "    im = ax4.imshow(first_image, cmap='hot', interpolation='nearest')\n",
    "    ax4.set_title(f'第一个样本 (标签: {y[0]})')\n",
    "    ax4.axis('off')\n",
    "    plt.colorbar(im, ax=ax4, fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # 5. 不同数字的像素值对比\n",
    "    ax5 = axes[1, 1]\n",
    "    digit_means = []\n",
    "    digit_stds = []\n",
    "    for digit in range(10):\n",
    "        mask = y == digit\n",
    "        digit_pixels = X[mask]\n",
    "        digit_means.append(digit_pixels.mean())\n",
    "        digit_stds.append(digit_pixels.std())\n",
    "    \n",
    "    x_pos = np.arange(10)\n",
    "    width = 0.35\n",
    "    ax5.bar(x_pos - width/2, digit_means, width, label='平均值', alpha=0.7)\n",
    "    ax5.bar(x_pos + width/2, digit_stds, width, label='标准差', alpha=0.7)\n",
    "    ax5.set_title('不同数字的像素统计')\n",
    "    ax5.set_xlabel('数字')\n",
    "    ax5.set_ylabel('像素值')\n",
    "    ax5.set_xticks(x_pos)\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. 样本展示\n",
    "    ax6 = axes[1, 2]\n",
    "    for i in range(10):\n",
    "        idx = np.where(y == i)[0][0]  # 找到每个数字的第一个样本\n",
    "        plt.subplot(2, 5, i+1)\n",
    "        plt.imshow(X[idx].reshape(28, 28), cmap='gray')\n",
    "        plt.title(f'数字 {i}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 可视化数据分布\n",
    "visualize_data_distribution(X_raw, y_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 数据预处理方法对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_preprocessing_methods(X, sample_size=1000):\n",
    "    \"\"\"对比不同的预处理方法\"\"\"\n",
    "    print(\"\\n=== 数据预处理方法对比 ===\")\n",
    "    \n",
    "    # 随机采样以提高计算效率\n",
    "    indices = np.random.choice(len(X), min(sample_size, len(X)), replace=False)\n",
    "    X_sample = X[indices]\n",
    "    \n",
    "    # 1. 原始数据\n",
    "    X_original = X_sample.copy()\n",
    "    \n",
    "    # 2. 归一化到[0,1]\n",
    "    X_normalized = X_sample / 255.0\n",
    "    \n",
    "    # 3. 标准化 (Z-score)\n",
    "    scaler_standard = StandardScaler()\n",
    "    X_standardized = scaler_standard.fit_transform(X_sample)\n",
    "    \n",
    "    # 4. Min-Max缩放到[0,1]\n",
    "    scaler_minmax = MinMaxScaler()\n",
    "    X_minmax = scaler_minmax.fit_transform(X_sample)\n",
    "    \n",
    "    # 5. 中心化 (减去均值)\n",
    "    X_centered = X_sample - X_sample.mean(axis=0)\n",
    "    \n",
    "    methods = {\n",
    "        '原始数据': X_original,\n",
    "        '归一化[0,1]': X_normalized,\n",
    "        '标准化(Z-score)': X_standardized,\n",
    "        'Min-Max缩放': X_minmax,\n",
    "        '中心化': X_centered\n",
    "    }\n",
    "    \n",
    "    # 对比统计信息\n",
    "    print(\"\\n预处理方法统计对比:\")\n",
    "    print(\"方法\\t\\t平均值\\t标准差\\t最小值\\t最大值\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for method_name, data in methods.items():\n",
    "        print(f\"{method_name:<15}\\t{data.mean():.3f}\\t{data.std():.3f}\\t{data.min():.3f}\\t{data.max():.3f}\")\n",
    "    \n",
    "    # 可视化对比\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (method_name, data) in enumerate(methods.items()):\n",
    "        if i >= 5:\n",
    "            break\n",
    "            \n",
    "        # 显示第一个样本\n",
    "        axes[i].imshow(data[0].reshape(28, 28), cmap='gray')\n",
    "        axes[i].set_title(f'{method_name}\\n(值域: [{data.min():.2f}, {data.max():.2f}])')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    # 最后一个图显示像素值分布对比\n",
    "    axes[5].set_title('像素值分布对比')\n",
    "    for method_name, data in list(methods.items())[:3]:  # 只显示前3种方法避免过于拥挤\n",
    "        axes[5].hist(data.flatten(), bins=50, alpha=0.5, label=method_name, density=True)\n",
    "    axes[5].set_xlabel('像素值')\n",
    "    axes[5].set_ylabel('密度')\n",
    "    axes[5].legend()\n",
    "    axes[5].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return methods\n",
    "\n",
    "# 对比预处理方法\n",
    "preprocessing_methods = compare_preprocessing_methods(X_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 数据分割策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_strategically(X, y, test_size=0.2, val_size=0.1, random_state=42):\n",
    "    \"\"\"战略性数据分割，保持标签分布\"\"\"\n",
    "    print(f\"\\n=== 数据分割策略 ===\")\n",
    "    print(f\"原始数据集大小: {X.shape}\")\n",
    "    print(f\"测试集比例: {test_size}\")\n",
    "    print(f\"验证集比例: {val_size}\")\n",
    "    \n",
    "    # 首先分割出测试集\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state, \n",
    "        stratify=y  # 分层采样保持标签分布\n",
    "    )\n",
    "    \n",
    "    # 再从剩余数据中分割出验证集\n",
    "    val_size_adjusted = val_size / (1 - test_size)  # 调整验证集比例\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp,\n",
    "        test_size=val_size_adjusted,\n",
    "        random_state=random_state,\n",
    "        stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n分割结果:\")\n",
    "    print(f\"训练集: {X_train.shape} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "    print(f\"验证集: {X_val.shape} ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "    print(f\"测试集: {X_test.shape} ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "    \n",
    "    # 检查每个数据集的标签分布\n",
    "    print(f\"\\n各数据集标签分布:\")\n",
    "    datasets = [\n",
    "        (\"训练集\", y_train),\n",
    "        (\"验证集\", y_val),\n",
    "        (\"测试集\", y_test)\n",
    "    ]\n",
    "    \n",
    "    for name, labels in datasets:\n",
    "        unique, counts = np.unique(labels, return_counts=True)\n",
    "        distribution = counts / len(labels) * 100\n",
    "        print(f\"\\n{name}:\")\n",
    "        for digit, count, pct in zip(unique, counts, distribution):\n",
    "            print(f\"  数字 {digit}: {count:4d} ({pct:5.1f}%)\")\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# 数据分割\n",
    "X_train_raw, X_val_raw, X_test_raw, y_train_raw, y_val_raw, y_test_raw = split_data_strategically(X_raw, y_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 特征工程和降维分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance_and_pca(X, y, n_components=50):\n",
    "    \"\"\"分析特征重要性和PCA降维\"\"\"\n",
    "    print(f\"\\n=== 特征分析和PCA降维 ===\")\n",
    "    \n",
    "    # 数据标准化\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # PCA分析\n",
    "    print(f\"\\nPCA分析 (前{n_components}个主成分):\")\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # 解释方差比例\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "    \n",
    "    print(f\"前10个主成分解释方差比例:\")\n",
    "    for i in range(min(10, n_components)):\n",
    "        print(f\"  PC{i+1}: {explained_variance_ratio[i]:.4f} (累计: {cumulative_variance_ratio[i]:.4f})\")\n",
    "    \n",
    "    # 找到保留95%方差所需的组件数\n",
    "    n_components_95 = np.argmax(cumulative_variance_ratio >= 0.95) + 1\n",
    "    print(f\"\\n保留95%方差需要 {n_components_95} 个主成分\")\n",
    "    print(f\"原始特征数: {X.shape[1]}\")\n",
    "    print(f\"降维比例: {(1 - n_components_95/X.shape[1])*100:.1f}%\")\n",
    "    \n",
    "    # 可视化PCA结果\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. 解释方差比例图\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.plot(range(1, n_components + 1), cumulative_variance_ratio[:n_components], 'bo-')\n",
    "    ax1.axhline(y=0.95, color='r', linestyle='--', alpha=0.7, label='95%方差')\n",
    "    ax1.axhline(y=0.90, color='orange', linestyle='--', alpha=0.7, label='90%方差')\n",
    "    ax1.set_xlabel('主成分数量')\n",
    "    ax1.set_ylabel('累计解释方差比例')\n",
    "    ax1.set_title('PCA累计解释方差')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. 前几个主成分的贡献度\n",
    "    ax2 = axes[0, 1]\n",
    "    n_show = min(20, n_components)\n",
    "    ax2.bar(range(1, n_show + 1), explained_variance_ratio[:n_show])\n",
    "    ax2.set_xlabel('主成分')\n",
    "    ax2.set_ylabel('解释方差比例')\n",
    "    ax2.set_title(f'前{n_show}个主成分的贡献度')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. 2D PCA可视化 (前两个主成分)\n",
    "    ax3 = axes[1, 0]\n",
    "    scatter = ax3.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab10', alpha=0.6, s=1)\n",
    "    ax3.set_xlabel(f'PC1 (解释方差: {explained_variance_ratio[0]:.3f})')\n",
    "    ax3.set_ylabel(f'PC2 (解释方差: {explained_variance_ratio[1]:.3f})')\n",
    "    ax3.set_title('前两个主成分的2D可视化')\n",
    "    plt.colorbar(scatter, ax=ax3)\n",
    "    \n",
    "    # 4. 像素重要性分析 (方差)\n",
    "    ax4 = axes[1, 1]\n",
    "    pixel_variance = np.var(X_scaled, axis=0)\n",
    "    variance_image = pixel_variance.reshape(28, 28)\n",
    "    im = ax4.imshow(variance_image, cmap='hot')\n",
    "    ax4.set_title('像素方差分布 (特征重要性)')\n",
    "    ax4.axis('off')\n",
    "    plt.colorbar(im, ax=ax4, fraction=0.046, pad=0.04)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return pca, X_pca, scaler\n",
    "\n",
    "# 使用训练数据进行分析\n",
    "pca_model, X_pca_analysis, scaler_analysis = analyze_feature_importance_and_pca(X_train_raw, y_train_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 最终数据预处理流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_preprocessing_pipeline(X_train, X_val, X_test, y_train, y_val, y_test, \n",
    "                                normalization_method='minmax', \n",
    "                                apply_pca=False,\n",
    "                                n_components=None):\n",
    "    \"\"\"最终的数据预处理流水线\"\"\"\n",
    "    print(f\"\\n=== 最终数据预处理流水线 ===\")\n",
    "    print(f\"归一化方法: {normalization_method}\")\n",
    "    print(f\"PCA降维: {apply_pca}\")\n",
    "    if apply_pca:\n",
    "        print(f\"PCA组件数: {n_components}\")\n",
    "    \n",
    "    # 1. 归一化/标准化\n",
    "    if normalization_method == 'minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "        print(\"使用MinMaxScaler归一化到[0,1]\")\n",
    "    elif normalization_method == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "        print(\"使用StandardScaler标准化\")\n",
    "    elif normalization_method == 'simple':\n",
    "        scaler = None\n",
    "        print(\"使用简单归一化 (除以255)\")\n",
    "    else:\n",
    "        raise ValueError(\"不支持的归一化方法\")\n",
    "    \n",
    "    # 应用归一化\n",
    "    if scaler is not None:\n",
    "        X_train_processed = scaler.fit_transform(X_train)\n",
    "        X_val_processed = scaler.transform(X_val)\n",
    "        X_test_processed = scaler.transform(X_test)\n",
    "    else:\n",
    "        X_train_processed = X_train / 255.0\n",
    "        X_val_processed = X_val / 255.0\n",
    "        X_test_processed = X_test / 255.0\n",
    "    \n",
    "    print(f\"归一化后统计:\")\n",
    "    print(f\"  训练集: 均值={X_train_processed.mean():.4f}, 标准差={X_train_processed.std():.4f}\")\n",
    "    print(f\"  验证集: 均值={X_val_processed.mean():.4f}, 标准差={X_val_processed.std():.4f}\")\n",
    "    print(f\"  测试集: 均值={X_test_processed.mean():.4f}, 标准差={X_test_processed.std():.4f}\")\n",
    "    \n",
    "    # 2. PCA降维 (可选)\n",
    "    if apply_pca:\n",
    "        if n_components is None:\n",
    "            # 自动选择组件数\n",
    "            pca_temp = PCA().fit(X_train_processed)\n",
    "            cumulative_variance = np.cumsum(pca_temp.explained_variance_ratio_)\n",
    "            n_components = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "            print(f\"自动选择PCA组件数: {n_components} (保留95%方差)\")\n",
    "        \n",
    "        pca = PCA(n_components=n_components)\n",
    "        X_train_processed = pca.fit_transform(X_train_processed)\n",
    "        X_val_processed = pca.transform(X_val_processed)\n",
    "        X_test_processed = pca.transform(X_test_processed)\n",
    "        \n",
    "        print(f\"PCA降维后特征数: {X_train_processed.shape[1]}\")\n",
    "        print(f\"解释方差比例: {pca.explained_variance_ratio_.sum():.4f}\")\n",
    "    else:\n",
    "        pca = None\n",
    "        print(f\"保持原始特征数: {X_train_processed.shape[1]}\")\n",
    "    \n",
    "    # 3. 标签处理 (One-hot编码)\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    y_train_encoded = encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "    y_val_encoded = encoder.transform(y_val.reshape(-1, 1))\n",
    "    y_test_encoded = encoder.transform(y_test.reshape(-1, 1))\n",
    "    \n",
    "    print(f\"\\n标签处理完成:\")\n",
    "    print(f\"  One-hot编码维度: {y_train_encoded.shape[1]}\")\n",
    "    print(f\"  编码示例 (数字{y_train[0]}): {y_train_encoded[0]}\")\n",
    "    \n",
    "    # 4. 最终数据验证\n",
    "    print(f\"\\n最终数据形状:\")\n",
    "    print(f\"  X_train: {X_train_processed.shape}\")\n",
    "    print(f\"  X_val:   {X_val_processed.shape}\")\n",
    "    print(f\"  X_test:  {X_test_processed.shape}\")\n",
    "    print(f\"  y_train: {y_train_encoded.shape}\")\n",
    "    print(f\"  y_val:   {y_val_encoded.shape}\")\n",
    "    print(f\"  y_test:  {y_test_encoded.shape}\")\n",
    "    \n",
    "    return {\n",
    "        'X_train': X_train_processed,\n",
    "        'X_val': X_val_processed,\n",
    "        'X_test': X_test_processed,\n",
    "        'y_train': y_train_encoded,\n",
    "        'y_val': y_val_encoded,\n",
    "        'y_test': y_test_encoded,\n",
    "        'scaler': scaler,\n",
    "        'pca': pca,\n",
    "        'encoder': encoder\n",
    "    }\n",
    "\n",
    "# 应用最终预处理流程\n",
    "processed_data = final_preprocessing_pipeline(\n",
    "    X_train_raw, X_val_raw, X_test_raw,\n",
    "    y_train_raw, y_val_raw, y_test_raw,\n",
    "    normalization_method='simple',  # 简单归一化除以255\n",
    "    apply_pca=False  # 不使用PCA，保持完整特征\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 数据质量验证和保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_and_save_preprocessed_data(processed_data, save_path='processed_data/'):\n",
    "    \"\"\"验证预处理后的数据并保存\"\"\"\n",
    "    print(f\"\\n=== 数据验证和保存 ===\")\n",
    "    \n",
    "    # 1. 数据完整性验证\n",
    "    print(\"数据完整性检查:\")\n",
    "    \n",
    "    # 检查缺失值\n",
    "    for name in ['X_train', 'X_val', 'X_test']:\n",
    "        missing = np.isnan(processed_data[name]).sum()\n",
    "        print(f\"  {name} 缺失值: {missing}\")\n",
    "    \n",
    "    # 检查数据范围\n",
    "    X_train = processed_data['X_train']\n",
    "    print(f\"\\n数据范围检查:\")\n",
    "    print(f\"  训练集范围: [{X_train.min():.4f}, {X_train.max():.4f}]\")\n",
    "    print(f\"  数值类型: {X_train.dtype}\")\n",
    "    \n",
    "    # 2. 标签验证\n",
    "    y_train = processed_data['y_train']\n",
    "    print(f\"\\n标签验证:\")\n",
    "    print(f\"  One-hot编码正确性: {np.all(y_train.sum(axis=1) == 1)}\")\n",
    "    print(f\"  标签维度: {y_train.shape[1]}\")\n",
    "    \n",
    "    # 3. 数据分布验证\n",
    "    print(f\"\\n数据分布验证:\")\n",
    "    y_train_labels = np.argmax(y_train, axis=1)\n",
    "    unique, counts = np.unique(y_train_labels, return_counts=True)\n",
    "    print(f\"  训练集标签分布: {dict(zip(unique, counts))}\")\n",
    "    \n",
    "    # 4. 可视化预处理后的样本\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for digit in range(10):\n",
    "        # 找到每个数字的第一个样本\n",
    "        idx = np.where(y_train_labels == digit)[0][0]\n",
    "        \n",
    "        # 如果应用了PCA，无法显示原始图像\n",
    "        if processed_data['pca'] is not None:\n",
    "            # 显示PCA重构的近似图像（仅当PCA组件数足够时）\n",
    "            if processed_data['pca'].n_components_ >= 50:\n",
    "                # 简单可视化：显示前几个像素或主成分\n",
    "                sample_data = X_train[idx][:min(100, len(X_train[idx]))]\n",
    "                if len(sample_data) >= 25:\n",
    "                    img = sample_data[:25].reshape(5, 5)\n",
    "                    axes[digit].imshow(img, cmap='viridis')\n",
    "                else:\n",
    "                    axes[digit].text(0.5, 0.5, f'PC数据\\n数字{digit}', \n",
    "                                   ha='center', va='center', transform=axes[digit].transAxes)\n",
    "            else:\n",
    "                axes[digit].text(0.5, 0.5, f'PC数据\\n数字{digit}', \n",
    "                               ha='center', va='center', transform=axes[digit].transAxes)\n",
    "        else:\n",
    "            # 显示原始图像\n",
    "            img = X_train[idx].reshape(28, 28)\n",
    "            axes[digit].imshow(img, cmap='gray')\n",
    "        \n",
    "        axes[digit].set_title(f'预处理后\\n数字 {digit}')\n",
    "        axes[digit].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 5. 保存预处理数据\n",
    "    import os\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    # 保存主要数据\n",
    "    np.save(f'{save_path}X_train.npy', processed_data['X_train'])\n",
    "    np.save(f'{save_path}X_val.npy', processed_data['X_val'])\n",
    "    np.save(f'{save_path}X_test.npy', processed_data['X_test'])\n",
    "    np.save(f'{save_path}y_train.npy', processed_data['y_train'])\n",
    "    np.save(f'{save_path}y_val.npy', processed_data['y_val'])\n",
    "    np.save(f'{save_path}y_test.npy', processed_data['y_test'])\n",
    "    \n",
    "    # 保存预处理器\n",
    "    import pickle\n",
    "    with open(f'{save_path}preprocessors.pkl', 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'scaler': processed_data['scaler'],\n",
    "            'pca': processed_data['pca'],\n",
    "            'encoder': processed_data['encoder']\n",
    "        }, f)\n",
    "    \n",
    "    print(f\"\\n数据已保存到: {save_path}\")\n",
    "    \n",
    "    # 保存数据信息\n",
    "    data_info = {\n",
    "        'X_train_shape': processed_data['X_train'].shape,\n",
    "        'X_val_shape': processed_data['X_val'].shape,\n",
    "        'X_test_shape': processed_data['X_test'].shape,\n",
    "        'y_train_shape': processed_data['y_train'].shape,\n",
    "        'normalization_method': 'simple_divide_255',\n",
    "        'pca_applied': processed_data['pca'] is not None,\n",
    "        'feature_count': processed_data['X_train'].shape[1]\n",
    "    }\n",
    "    \n",
    "    with open(f'{save_path}data_info.pkl', 'wb') as f:\n",
    "        pickle.dump(data_info, f)\n",
    "    \n",
    "    print(f\"数据信息: {data_info}\")\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "# 验证和保存数据\n",
    "final_processed_data = validate_and_save_preprocessed_data(processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 预处理总结和建议"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_summary():\n",
    "    \"\"\"预处理总结和建议\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"           数据预处理总结报告\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\n【数据集概况】\")\n",
    "    print(\"  - 原始数据: MNIST手写数字数据集\")\n",
    "    print(\"  - 图像尺寸: 28×28像素 (784个特征)\")\n",
    "    print(\"  - 样本总数: 70,000\")\n",
    "    print(\"  - 数字类别: 0-9 (10个类别)\")\n",
    "    print(\"  - 数据质量: 无缺失值，标签平衡\")\n",
    "    \n",
    "    print(\"\\n【预处理步骤】\")\n",
    "    print(\"  1. 数据分割: 训练集(70%) / 验证集(10%) / 测试集(20%)\")\n",
    "    print(\"  2. 归一化: 简单归一化 (像素值÷255)\")\n",
    "    print(\"  3. 标签编码: One-hot编码\")\n",
    "    print(\"  4. 数据验证: 完整性、范围、分布检查\")\n",
    "    \n",
    "    print(\"\\n【预处理方法对比】\")\n",
    "    print(\"  方法              优点                    缺点\")\n",
    "    print(\"  \" + \"-\"*55)\n",
    "    print(\"  简单归一化        保持数据分布，计算简单    可能不是最优分布\")\n",
    "    print(\"  标准化(Z-score)   零均值单位方差          可能改变原始分布\")\n",
    "    print(\"  Min-Max缩放       固定范围[0,1]          对异常值敏感\")\n",
    "    print(\"  PCA降维           减少计算复杂度          可能丢失信息\")\n",
    "    \n",
    "    print(\"\\n【最终选择】\")\n",
    "    print(\"  - 归一化: 简单归一化 (除以255)\")\n",
    "    print(\"  - 降维: 不使用PCA (保留完整特征)\")\n",
    "    print(\"  - 理由: 保持原始信息，适合神经网络学习\")\n",
    "    \n",
    "    print(\"\\n【性能优化建议】\")\n",
    "    print(\"  1. 数据增强: 旋转、平移、缩放可提升泛化能力\")\n",
    "    print(\"  2. 批量处理: 使用适当批量大小提升训练效率\")\n",
    "    print(\"  3. 特征选择: 基于方差或相关性选择重要特征\")\n",
    "    print(\"  4. 正则化: 防止过拟合的技术\")\n",
    "    \n",
    "    print(\"\\n【神经网络训练建议】\")\n",
    "    print(\"  1. 学习率: 从0.001开始，根据训练情况调整\")\n",
    "    print(\"  2. 批量大小: 32-128之间，根据内存情况选择\")\n",
    "    print(\"  3. 网络结构: 从简单开始，逐步增加复杂度\")\n",
    "    print(\"  4. 早停策略: 监控验证集损失，防止过拟合\")\n",
    "    \n",
    "    print(\"\\n【预期性能】\")\n",
    "    print(\"  - 基准准确率: >95% (简单神经网络)\")\n",
    "    print(\"  - 优化准确率: >98% (深度网络+正则化)\")\n",
    "    print(\"  - 训练时间: 10-60分钟 (取决于硬件配置)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"数据预处理完成，准备进行神经网络训练！\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# 显示预处理总结\n",
    "preprocessing_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 数据加载函数（供其他notebook使用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_data(data_path='processed_data/'):\n",
    "    \"\"\"加载预处理好的数据\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): 预处理数据保存路径\n",
    "    \n",
    "    Returns:\n",
    "        dict: 包含所有预处理数据和预处理器\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pickle\n",
    "    \n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"预处理数据路径不存在: {data_path}\")\n",
    "    \n",
    "    # 加载数据\n",
    "    data = {\n",
    "        'X_train': np.load(f'{data_path}X_train.npy'),\n",
    "        'X_val': np.load(f'{data_path}X_val.npy'),\n",
    "        'X_test': np.load(f'{data_path}X_test.npy'),\n",
    "        'y_train': np.load(f'{data_path}y_train.npy'),\n",
    "        'y_val': np.load(f'{data_path}y_val.npy'),\n",
    "        'y_test': np.load(f'{data_path}y_test.npy')\n",
    "    }\n",
    "    \n",
    "    # 加载预处理器\n",
    "    with open(f'{data_path}preprocessors.pkl', 'rb') as f:\n",
    "        preprocessors = pickle.load(f)\n",
    "    \n",
    "    data.update(preprocessors)\n",
    "    \n",
    "    # 加载数据信息\n",
    "    with open(f'{data_path}data_info.pkl', 'rb') as f:\n",
    "        data_info = pickle.load(f)\n",
    "    \n",
    "    print(f\"成功加载预处理数据:\")\n",
    "    for key, value in data_info.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# 示例使用（注释掉避免实际执行）\n",
    "# loaded_data = load_preprocessed_data()\n",
    "# print(f\"加载的训练数据形状: {loaded_data['X_train'].shape}\")\n",
    "\n",
    "print(\"\\n数据预处理notebook已完成！\")\n",
    "print(\"主要成果:\")\n",
    "print(\"1. 完整的MNIST数据加载和探索\")\n",
    "print(\"2. 多种预处理方法对比分析\")\n",
    "print(\"3. 战略性数据分割保持标签分布\")\n",
    "print(\"4. PCA降维分析（保留95%方差需约150个主成分）\")\n",
    "print(\"5. 最终预处理流水线和数据验证\")\n",
    "print(\"6. 数据保存和加载功能\")\n",
    "print(\"\\n数据已准备就绪，可以开始神经网络训练！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}