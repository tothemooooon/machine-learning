# 感知机与神经网络教程

## 目录
1. [感知机基础理论](#感知机基础理论)
2. [感知机算法实现](#感知机算法实现)
3. [神经网络基础](#神经网络基础)
4. [实践案例](#实践案例)
5. [常见问题与解决方案](#常见问题与解决方案)

## 感知机基础理论

### 1.1 什么是感知机
感知机（Perceptron）是神经网络和支持向量机的基础，是二分类的线性分类模型。由Frank Rosenblatt于1957年提出。

### 1.2 模型定义
- **输入**: 特征向量 x = (x₁, x₂, ..., xₙ)
- **权重**: w = (w₁, w₂, ..., wₙ)
- **偏置**: b
- **输出**: y = sign(w·x + b)

其中sign函数为：
```
sign(z) = {
  1,  if z ≥ 0
 -1,  if z < 0
}
```

### 1.3 几何解释
感知机对应于特征空间中的一个分离超平面：
w·x + b = 0

- w：超平面的法向量
- b：超平面的截距

### 1.4 学习策略
**损失函数**: 误分类点到超平面的总距离
```
L(w,b) = -∑(yᵢ(w·xᵢ + b))
```
其中求和遍历所有误分类点。

**学习算法**: 随机梯度下降
```
w ← w + ηyᵢxᵢ
b ← b + ηyᵢ
```
其中η为学习率。

## 感知机算法实现

### 2.1 原始形式算法
```python
import numpy as np

class Perceptron:
    def __init__(self, learning_rate=0.1, max_iter=1000):
        self.lr = learning_rate
        self.max_iter = max_iter
        self.w = None
        self.b = None

    def fit(self, X, y):
        # 初始化参数
        n_samples, n_features = X.shape
        self.w = np.zeros(n_features)
        self.b = 0

        # 梯度下降
        for _ in range(self.max_iter):
            for i in range(n_samples):
                # 预测
                y_pred = np.sign(np.dot(self.w, X[i]) + self.b)

                # 如果预测错误，更新参数
                if y_pred != y[i]:
                    self.w += self.lr * y[i] * X[i]
                    self.b += self.lr * y[i]

    def predict(self, X):
        return np.sign(np.dot(X, self.w) + self.b)
```

### 2.2 对偶形式算法
```python
class PerceptronDual:
    def __init__(self, learning_rate=0.1, max_iter=1000):
        self.lr = learning_rate
        self.max_iter = max_iter
        self.alpha = None
        self.b = None
        self.gram_matrix = None

    def fit(self, X, y):
        n_samples = X.shape[0]
        self.alpha = np.zeros(n_samples)
        self.b = 0

        # 计算Gram矩阵
        self.gram_matrix = np.dot(X, X.T)

        for _ in range(self.max_iter):
            for i in range(n_samples):
                # 计算预测值
                prediction = np.sum(self.alpha * y * self.gram_matrix[i]) + self.b
                y_pred = np.sign(prediction)

                # 更新参数
                if y_pred != y[i]:
                    self.alpha[i] += self.lr
                    self.b += self.lr * y[i]

    def predict(self, X):
        w = np.sum(self.alpha.reshape(-1,1) * self.y.reshape(-1,1) * self.X, axis=0)
        return np.sign(np.dot(X, w) + self.b)
```

## 神经网络基础

### 3.1 从感知机到神经网络
单层感知机的局限性：
- 只能解决线性可分问题
- 无法解决XOR等非线性问题

多层感知机（MLP）通过：
- 隐藏层实现特征变换
- 非线性激活函数
- 反向传播算法

### 3.2 多层感知机结构
```
输入层 → 隐藏层 → 输出层
```

**前向传播**：
```
h = f(W₁x + b₁)
y = g(W₂h + b₂)
```

其中f、g为激活函数。

### 3.3 常用激活函数
```python
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def tanh(x):
    return np.tanh(x)

def relu(x):
    return np.maximum(0, x)

def leaky_relu(x, alpha=0.01):
    return np.maximum(alpha * x, x)
```

### 3.4 反向传播算法
```python
class SimpleNeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size):
        # 初始化权重
        self.W1 = np.random.randn(input_size, hidden_size) * 0.01
        self.b1 = np.zeros((1, hidden_size))
        self.W2 = np.random.randn(hidden_size, output_size) * 0.01
        self.b2 = np.zeros((1, output_size))

    def forward(self, X):
        # 前向传播
        self.z1 = np.dot(X, self.W1) + self.b1
        self.a1 = sigmoid(self.z1)
        self.z2 = np.dot(self.a1, self.W2) + self.b2
        self.a2 = sigmoid(self.z2)
        return self.a2

    def backward(self, X, y, learning_rate):
        # 反向传播
        m = X.shape[0]

        # 输出层梯度
        dz2 = self.a2 - y
        dW2 = (1/m) * np.dot(self.a1.T, dz2)
        db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)

        # 隐藏层梯度
        dz1 = np.dot(dz2, self.W2.T) * self.a1 * (1 - self.a1)
        dW1 = (1/m) * np.dot(X.T, dz1)
        db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)

        # 参数更新
        self.W1 -= learning_rate * dW1
        self.b1 -= learning_rate * db1
        self.W2 -= learning_rate * dW2
        self.b2 -= learning_rate * db2
```

## 实践案例

### 4.1 AND逻辑门实现
```python
# AND逻辑门数据
X = np.array([[1, 1], [1, 0], [0, 1], [0, 0]])
y = np.array([1, -1, -1, -1])

# 训练感知机
perceptron = Perceptron(learning_rate=0.1, max_iter=100)
perceptron.fit(X, y)

# 测试
test_points = np.array([[1, 1], [1, 0], [0, 1], [0, 0]])
predictions = perceptron.predict(test_points)
print("AND逻辑门预测结果:", predictions)
```

### 4.2 XOR问题（需要多层网络）
```python
# XOR数据
X = np.array([[1, 1], [1, 0], [0, 1], [0, 0]])
y = np.array([[-1], [1], [1], [-1]])  # 注意标签调整

# 创建神经网络
nn = SimpleNeuralNetwork(input_size=2, hidden_size=4, output_size=1)

# 训练
for epoch in range(10000):
    output = nn.forward(X)
    nn.backward(X, y, learning_rate=0.1)

    if epoch % 1000 == 0:
        loss = np.mean((output - y) ** 2)
        print(f"Epoch {epoch}, Loss: {loss:.4f}")

# 测试
predictions = nn.forward(X)
print("XOR预测结果:")
for i in range(len(X)):
    print(f"Input: {X[i]}, Predicted: {predictions[i][0]:.4f}, Actual: {y[i][0]}")
```

### 4.3 鸢尾花分类
```python
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# 加载数据
iris = load_iris()
X = iris.data
y = iris.target

# 二分类问题：只取前两类
X = X[y != 2]
y = y[y != 2]
y = np.where(y == 0, -1, 1)  # 转换为{-1, 1}

# 标准化
scaler = StandardScaler()
X = scaler.fit_transform(X)

# 分割数据
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 训练感知机
perceptron = Perceptron(learning_rate=0.01, max_iter=1000)
perceptron.fit(X_train, y_train)

# 评估
predictions = perceptron.predict(X_test)
accuracy = np.mean(predictions == y_test)
print(f"鸢尾花分类准确率: {accuracy:.4f}")
```

## 常见问题与解决方案

### 5.1 收敛性问题
**问题**: 感知机不收敛
**原因**: 数据线性不可分
**解决**:
- 使用多层感知机
- 添加特征变换
- 使用核方法

### 5.2 学习率选择
**经验法则**:
- 太小：收敛慢
- 太大：震荡或发散
- 建议：0.01, 0.1, 1.0之间尝试

### 5.3 特征缩放
```python
# 重要：使用前进行特征缩放
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

### 5.4 过拟合预防
- 早停法
- 正则化
- 交叉验证

## 扩展阅读

### 6.1 相关算法
- [支持向量机(SVM)](./SVM教程.md)
- [逻辑回归](./逻辑回归教程.md)
- [深度学习](./深度学习教程.md)

### 6.2 数学基础
- 线性代数：向量、矩阵运算
- 微积分：梯度、偏导数
- 优化理论：凸优化、梯度下降

### 6.3 实践建议
1. 从简单问题开始
2. 可视化决策边界
3. 对比不同算法性能
4. 理解超参数影响

## 参考资源
- 《统计学习方法》- 李航
- 《模式识别与机器学习》- Christopher Bishop
- [CS231n: Convolutional Neural Networks](http://cs231n.stanford.edu/)
- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)

---

*最后更新: 2025年10月*
*作者: 机器学习课程笔记*